{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to perform double segmentation, a standard and an ROI model is needed\n",
    "### we will use the previously trained standard deeplab example and ROI deeplab example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import general_config\n",
    "import constants\n",
    "import numpy as np\n",
    "import save_complete_nii_results\n",
    "from utils import inference\n",
    "from utils import prepare_models\n",
    "from utils.training_utils.box_utils import convert_offsets_to_bboxes, wh2corners\n",
    "from utils.training_utils import training_processing\n",
    "from utils.ROI_crop import roi_crop\n",
    "from utils.ROI_crop import predict_ROI\n",
    "from utils.training_utils import prints, training_setup\n",
    "from utils import metrics\n",
    "from utils import visualization\n",
    "\n",
    "\n",
    "def get_double_seg_predictions(model_a, model_b, dataset, logsoft_preds=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    model_a: str -> model to extract ROI (standard)\n",
    "    model_b: str -> model to predict from ROI\n",
    "    dataset: str\n",
    "    \"\"\"\n",
    "    models, params_list, configs_list = prepare_models.prepare([model_a, model_b], dataset)\n",
    "    model_a, model_b = models\n",
    "    validation_loader = training_setup.prepare_val_loader(params_list[0], configs_list[0])\n",
    "    validation_loader_2 = training_setup.prepare_val_loader(params_list[1], configs_list[1])\n",
    "    \n",
    "    coords_n_scores = predict_ROI.get_segmentor_pred_coords(validation_loader, model_a, params_list[0], configs_list[0])\n",
    "    _, predictions_double_seg = predict_ROI.segment_with_computed_ROI(validation_loader_2, model_b, params_list[1], configs_list[1], coords_n_scores, logsoft_preds=logsoft_preds)\n",
    "    \n",
    "    return predictions_double_seg, validation_loader_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform double segmentation, add err_margin to standard model params.json\n",
    "_, _ = get_double_seg_predictions(\"Deeplab_standard_example\", \"Deeplab_ROI_example\", \"ACDC_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ensembling: softmax predictions of different models are summed and averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensembling two standard models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model weigths, again using the previously trained example models\n",
    "models, params_list, configs_list = prepare_models.prepare([\"Deeplab_standard_example\", \"2D_Unet_standard_example\"], \"ACDC_training\")\n",
    "model_a, model_b = models\n",
    "validation_loader_a = training_setup.prepare_val_loader(params_list[0], configs_list[0])\n",
    "validation_loader_b = training_setup.prepare_val_loader(params_list[1], configs_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference with both\n",
    "predictions_deeplab, _ = inference.run_model_inference(validation_loader_a, model_a, params_list[0], logsoft_preds=True)\n",
    "predictions_unet, _ = inference.run_model_inference(validation_loader_b, model_b, params_list[1], logsoft_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, ensemble standard models\n",
    "logsoftmax_volumes = [predictions_unet, predictions_deeplab]\n",
    "combined_volumes = inference.ensemble_inference(logsoftmax_volumes, validation_loader_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is also possible to ensemble standard and ROI models\n",
    "# add logsoft flag\n",
    "double_seg_preds, _ = get_double_seg_predictions(\"Deeplab_standard_example\", \"Deeplab_ROI_example\", \"ACDC_training\", logsoft_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble standard models and double segmentation output\n",
    "logsoftmax_volumes = [predictions_unet, predictions_deeplab, double_seg_preds]\n",
    "combined_volumes = inference.ensemble_inference(logsoftmax_volumes, validation_loader_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_double_seg_predictions_test(model_a, model_b, dataset):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    model_a: str -> model to extract ROI\n",
    "    model_b: str -> model to predict from ROI\n",
    "    dataset: str\n",
    "    \"\"\"\n",
    "    coords_n_scores = get_test_coords(model_a, dataset)\n",
    "    model_roi, params, config = prepare_models.prepare([model_b], dataset)\n",
    "    test_loader = training_setup.prepare_test_loader(params, config)\n",
    "    \n",
    "    predictions_double_seg = predict_ROI.segment_with_computed_ROI_test(test_loader, model_roi, params, config, coords_n_scores, logsoft_preds=True)\n",
    "    \n",
    "    return predictions_double_seg\n",
    "\n",
    "def get_test_coords(model, dataset):\n",
    "    model, params, config = prepare_models.prepare([model], dataset)\n",
    "    test_loader = training_setup.prepare_test_loader(params, config)\n",
    "    coords_n_scores = predict_ROI.get_segmentor_pred_coords(test_loader, model, params, config)\n",
    "    return coords_n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the test set is in a directory of its own, so copy/move the previously trained models' experiments folders\n",
    "# in experiments/acdc_test_set\n",
    "# also, don't forget to change the dataset name from ACDC_training to acdc_test_set in each config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running inference and ensemble on the test set is very similar to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models, params_list, configs_list = prepare_models.prepare([\"Deeplab_standard_example\", \"2D_Unet_standard_example\"], \"acdc_test_set\")\n",
    "model_a, model_b = models\n",
    "test_loader = training_setup.prepare_test_loader(params_list[0], configs_list[0])\n",
    "test_loader_2 = training_setup.prepare_test_loader(params_list[1], configs_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_deeplab = inference.run_model_test_inference(test_loader, model_a, params_list[0], logsoft_preds=True)\n",
    "predictions_unet = inference.run_model_test_inference(test_loader_2, model_b, params_list[1], logsoft_preds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_double = get_double_seg_predictions_test(\"Deeplab_standard_example\", \"Deeplab_ROI_example\", \"acdc_test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsoftmax_volumes = [predictions_unet, predictions_deeplab, predictions_double]\n",
    "combined_volumes = inference.ensemble_inference_test(logsoftmax_volumes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the results to a folder\n",
    "save_complete_nii_results.save_predictions_test(combined_volumes, \"ResNeXt_DeepLabV3_plus\", \"acdc_test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
